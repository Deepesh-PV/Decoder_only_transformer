Decoder-Only Transformer Demo

A small, study-purpose implementation of a Decoder-only Transformer, built with PyTorch.
It features self-attention and trains on a tiny dataset of 7 input-output pairs â€” making it ideal for learning and experimentation.
âœ¨ Features

    Decoder-Only Transformer architecture (no encoder).

    Self-Attention mechanism implemented using PyTorch.

    Trains on a very small dataset for fast and clear experimentation.

    Easily extensible to larger datasets and more complex tasks.

    Minimal, beginner-friendly code â€” ideal for studying Transformer internals.

ðŸ“š Dataset

    7 small, manually crafted input-output pairs.

    Designed to overfit quickly so you can observe the learning process.

    Dataset can easily be expanded for larger experiments.
