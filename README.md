Decoder-Only Transformer Demo

A small, study-purpose implementation of a Decoder-only Transformer, built with PyTorch.
It features self-attention and trains on a tiny dataset of 7 input-output pairs â€” making it ideal for learning and experimentation.
âœ¨ Features

    Decoder-Only Transformer architecture (no encoder).

    Self-Attention mechanism implemented using PyTorch.

    Trains on a very small dataset for fast and clear experimentation.

    Easily extensible to larger datasets and more complex tasks.

    Minimal, beginner-friendly code â€” ideal for studying Transformer internals.

ðŸ“š Dataset

    7 small, manually crafted input-output pairs.

    Designed to overfit quickly so you can observe the learning process.

    Dataset can easily be expanded for larger experiments.

ðŸ“ˆ Future Improvements

    Add multi-head self-attention (currently single-head for simplicity).

    Extend to bigger datasets or more complex sequence generation tasks.

    Experiment with different optimizers, schedulers, and loss functions.

    Add beam search or sampling for better sequence decoding.

![Made with PyTorch](https://img.shields.io/badge/Made%20with-PyTorch-red)
